{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02 ‚Ä¢ Prompt Engineering Lab\n",
    "\n",
    "**Goal:** Experiment with different prompting techniques and learn to evaluate their effectiveness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We'll use the OpenAI API (or any compatible endpoint). Make sure you have your API key set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install openai python-dotenv pandas\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Initialize client\n",
    "# Option 1: Use environment variable\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Option 2: For local LLMs (Ollama, LM Studio, etc.)\n",
    "# client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Let's create some utilities for making API calls and tracking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(\n",
    "    messages: List[Dict],\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 500,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Call the LLM and return response with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'content', 'tokens_in', 'tokens_out', 'latency_ms'\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    latency_ms = (time.time() - start) * 1000\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"tokens_in\": response.usage.prompt_tokens,\n",
    "        \"tokens_out\": response.usage.completion_tokens,\n",
    "        \"latency_ms\": round(latency_ms, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "def simple_prompt(user_input: str, system: str = \"You are a helpful assistant.\") -> Dict:\n",
    "    \"\"\"Simple single-turn prompt.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "    return call_llm(messages)\n",
    "\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-shot vs Few-shot Prompting\n",
    "\n",
    "### Zero-shot\n",
    "Asking the model to do something without examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot sentiment classification\n",
    "result = simple_prompt(\n",
    "    user_input=\"Classify the sentiment of this review as positive, negative, or neutral:\\n\\n\"\n",
    "                \"'The product arrived late but the quality exceeded my expectations.'\",\n",
    "    system=\"You are a sentiment classifier. Respond with only: POSITIVE, NEGATIVE, or NEUTRAL.\",\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['content']}\")\n",
    "print(f\"Tokens: {result['tokens_in']} in / {result['tokens_out']} out\")\n",
    "print(f\"Latency: {result['latency_ms']}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot\n",
    "Providing examples in the prompt to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt with examples\n",
    "few_shot_system = \"\"\"You are a sentiment classifier. Classify reviews as POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "\n",
    "Examples:\n",
    "Review: \"Love this! Best purchase ever.\"\n",
    "Sentiment: POSITIVE\n",
    "\n",
    "Review: \"Terrible quality, broke after one day.\"\n",
    "Sentiment: NEGATIVE\n",
    "\n",
    "Review: \"It's okay. Does what it says.\"\n",
    "Sentiment: NEUTRAL\n",
    "\n",
    "Review: \"Mixed feelings. Great features but poor support.\"\n",
    "Sentiment: NEUTRAL\n",
    "\n",
    "Now classify the following review. Respond with only the sentiment.\"\"\"\n",
    "\n",
    "result = simple_prompt(\n",
    "    user_input=\"The product arrived late but the quality exceeded my expectations.\",\n",
    "    system=few_shot_system,\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['content']}\")\n",
    "print(f\"Tokens: {result['tokens_in']} in / {result['tokens_out']} out\")\n",
    "print(f\"Latency: {result['latency_ms']}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain-of-Thought Prompting\n",
    "\n",
    "Asking the model to \"think step by step\" can improve reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"A store sells apples for $2 each. If you buy 7 apples and pay with a $20 bill, how much change do you get?\"\n",
    "\n",
    "# Without CoT\n",
    "result_direct = simple_prompt(\n",
    "    user_input=problem,\n",
    "    system=\"You are a math helper. Give the answer directly.\",\n",
    ")\n",
    "print(\"Direct answer:\")\n",
    "print(result_direct['content'])\n",
    "print()\n",
    "\n",
    "# With Chain-of-Thought\n",
    "result_cot = simple_prompt(\n",
    "    user_input=problem,\n",
    "    system=\"You are a math helper. Think step by step, showing your work. Then give the final answer.\",\n",
    ")\n",
    "print(\"Chain-of-Thought:\")\n",
    "print(result_cot['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temperature and Sampling\n",
    "\n",
    "Temperature controls randomness. Let's see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a one-sentence tagline for a coffee shop.\"\n",
    "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"Same prompt, different temperatures:\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    result = call_llm(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {result['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structured Output (JSON)\n",
    "\n",
    "Getting structured data from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt = \"\"\"Extract information from this text and return ONLY valid JSON:\n",
    "\n",
    "Text: \"John Smith is a 32-year-old software engineer from Seattle who loves hiking and coffee.\"\n",
    "\n",
    "Return JSON with keys: name, age, occupation, city, hobbies (array)\n",
    "Example format: {\"name\": \"...\", \"age\": 0, \"occupation\": \"...\", \"city\": \"...\", \"hobbies\": [\"...\"]}\n",
    "\"\"\"\n",
    "\n",
    "result = simple_prompt(json_prompt, system=\"You return only valid JSON, no other text.\")\n",
    "\n",
    "print(\"Raw response:\")\n",
    "print(result['content'])\n",
    "print()\n",
    "\n",
    "# Try to parse it\n",
    "try:\n",
    "    data = json.loads(result['content'])\n",
    "    print(\"Parsed successfully:\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Failed to parse: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Sentiment Classification Benchmark\n",
    "\n",
    "Let's build a proper benchmark to test different prompts on a labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample labeled dataset\n",
    "TEST_DATA = [\n",
    "    {\"text\": \"This is the best product I've ever bought!\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Completely disappointed. Waste of money.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"It works as expected.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Amazing customer service, very helpful!\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"The item arrived damaged.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Average product, nothing special.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Highly recommend to everyone!\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Never buying from here again.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Does the job, I guess.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Exceeded all my expectations!\", \"label\": \"POSITIVE\"},\n",
    "]\n",
    "\n",
    "def evaluate_prompt(test_data, system_prompt, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a prompt on test data.\n",
    "    Returns accuracy and details.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        result = simple_prompt(\n",
    "            user_input=f\"Review: {item['text']}\",\n",
    "            system=system_prompt,\n",
    "        )\n",
    "        \n",
    "        prediction = result['content'].strip().upper()\n",
    "        is_correct = prediction == item['label']\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": item['text'][:40],\n",
    "            \"expected\": item['label'],\n",
    "            \"predicted\": prediction,\n",
    "            \"correct\": is_correct,\n",
    "            \"tokens\": result['tokens_in'] + result['tokens_out'],\n",
    "        })\n",
    "        \n",
    "        if verbose:\n",
    "            status = \"‚úì\" if is_correct else \"‚úó\"\n",
    "            print(f\"{status} | Expected: {item['label']} | Got: {prediction}\")\n",
    "    \n",
    "    accuracy = correct / len(test_data)\n",
    "    total_tokens = sum(r['tokens'] for r in results)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": len(test_data),\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Evaluation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompt strategies\n",
    "prompts = {\n",
    "    \"Zero-shot (minimal)\": \"Classify the sentiment. Respond only: POSITIVE, NEGATIVE, or NEUTRAL.\",\n",
    "    \n",
    "    \"Zero-shot (detailed)\": \"\"\"You are a sentiment classifier.\n",
    "Analyze the given review text and classify its overall sentiment.\n",
    "Consider the tone, words used, and implied emotion.\n",
    "Respond with ONLY one word: POSITIVE, NEGATIVE, or NEUTRAL.\"\"\",\n",
    "    \n",
    "    \"Few-shot\": \"\"\"Classify sentiment. Examples:\n",
    "\"Love this!\" -> POSITIVE\n",
    "\"Terrible experience\" -> NEGATIVE\n",
    "\"It's okay\" -> NEUTRAL\n",
    "Now classify. Respond only: POSITIVE, NEGATIVE, or NEUTRAL.\"\"\",\n",
    "}\n",
    "\n",
    "# Run evaluations (commented out to save API calls - uncomment to run)\n",
    "# for name, prompt in prompts.items():\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Prompt: {name}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "#     eval_result = evaluate_prompt(TEST_DATA, prompt, verbose=True)\n",
    "#     print(f\"\\nAccuracy: {eval_result['accuracy']:.1%}\")\n",
    "#     print(f\"Total tokens: {eval_result['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building Your Prompt Catalog\n",
    "\n",
    "Create a collection of reusable prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_CATALOG = {\n",
    "    \"summarize\": {\n",
    "        \"system\": \"You are a concise summarizer. Capture the key points in 2-3 sentences.\",\n",
    "        \"template\": \"Summarize the following text:\\n\\n{text}\",\n",
    "        \"use_case\": \"Document summarization\",\n",
    "    },\n",
    "    \"extract-entities\": {\n",
    "        \"system\": \"You extract named entities. Return JSON with keys: people, organizations, locations, dates.\",\n",
    "        \"template\": \"Extract entities from:\\n\\n{text}\",\n",
    "        \"use_case\": \"Information extraction\",\n",
    "    },\n",
    "    \"code-review\": {\n",
    "        \"system\": \"You are a code reviewer. Focus on: bugs, performance, readability, security.\",\n",
    "        \"template\": \"Review this code and list issues/suggestions:\\n\\n```\\n{code}\\n```\",\n",
    "        \"use_case\": \"Code quality checks\",\n",
    "    },\n",
    "    \"explain-like-im-5\": {\n",
    "        \"system\": \"You explain complex topics simply, using analogies a 5-year-old would understand.\",\n",
    "        \"template\": \"Explain this concept: {topic}\",\n",
    "        \"use_case\": \"Educational content\",\n",
    "    },\n",
    "    \"rewrite-formal\": {\n",
    "        \"system\": \"You rewrite text to be more professional and formal while keeping the meaning.\",\n",
    "        \"template\": \"Rewrite this formally:\\n\\n{text}\",\n",
    "        \"use_case\": \"Professional communication\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save to JSON for reuse\n",
    "with open(\"../prompts/catalog.json\", \"w\") as f:\n",
    "    json.dump(PROMPT_CATALOG, f, indent=2)\n",
    "\n",
    "print(\"Prompt catalog saved!\")\n",
    "print(f\"\\nAvailable prompts: {list(PROMPT_CATALOG.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéØ Your Tasks\n",
    "\n",
    "### Task 1: Improve the Sentiment Classifier\n",
    "Try to get 100% accuracy on the test set. Experiment with:\n",
    "- Different system prompts\n",
    "- More few-shot examples\n",
    "- Chain-of-thought\n",
    "\n",
    "### Task 2: Create Your Own Prompt Template\n",
    "Add a new template to the catalog for a use case you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n",
    "\n",
    "# TODO: Create a prompt for a task you find useful\n",
    "my_prompt = {\n",
    "    \"system\": \"Your system prompt here\",\n",
    "    \"template\": \"Your template with {placeholders}\",\n",
    "    \"use_case\": \"What this is for\",\n",
    "}\n",
    "\n",
    "# Test it\n",
    "# result = simple_prompt(\n",
    "#     user_input=my_prompt[\"template\"].format(placeholder=\"your input\"),\n",
    "#     system=my_prompt[\"system\"],\n",
    "# )\n",
    "# print(result['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Track and Compare\n",
    "\n",
    "Create a comparison table of your prompt experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run your experiments and record results\n",
    "experiments = [\n",
    "    # {\"name\": \"Experiment 1\", \"accuracy\": 0.8, \"tokens\": 150, \"notes\": \"...\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(experiments)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üìù Reflection\n",
    "\n",
    "1. What prompt technique gave the best results for sentiment classification?\n",
    "2. How does temperature affect the consistency of outputs?\n",
    "3. What are the trade-offs between few-shot examples and prompt length?\n",
    "4. When would you use JSON output vs. free-form text?\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Continue to Module 03 to learn about fine-tuning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
