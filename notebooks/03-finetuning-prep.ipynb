{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03 ‚Ä¢ Fine-Tuning Preparation\n",
    "\n",
    "**Goal:** Prepare datasets, understand LoRA, and set up for fine-tuning experiments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install transformers datasets peft accelerate bitsandbytes torch\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Format\n",
    "\n",
    "Fine-tuning datasets typically follow the instruction format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset entries\n",
    "SAMPLE_DATASET = [\n",
    "    {\n",
    "        \"instruction\": \"Explain what a Docker container is.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A Docker container is a lightweight, standalone package that includes everything needed to run software: code, runtime, libraries, and settings. It's isolated from other containers and the host system.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert this time to 24-hour format.\",\n",
    "        \"input\": \"3:30 PM\",\n",
    "        \"output\": \"15:30\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a git commit message for these changes.\",\n",
    "        \"input\": \"Fixed login bug, updated README, added unit tests\",\n",
    "        \"output\": \"fix: resolve login authentication bug\\n\\n- Fix session timeout handling\\n- Update README with new API endpoints\\n- Add unit tests for auth module\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Sample entries: {len(SAMPLE_DATASET)}\")\n",
    "print(json.dumps(SAMPLE_DATASET[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Your Dataset\n",
    "\n",
    "Build a dataset for a specific use case. Let's create a DevOps Q&A dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DevOps Q&A dataset\n",
    "DEVOPS_QA = [\n",
    "    {\"instruction\": \"What is CI/CD?\", \"input\": \"\", \"output\": \"CI/CD stands for Continuous Integration and Continuous Deployment. CI automatically builds and tests code changes. CD automatically deploys validated changes to production. Together, they enable faster, safer software delivery.\"},\n",
    "    {\"instruction\": \"Explain Kubernetes pods.\", \"input\": \"\", \"output\": \"A Kubernetes pod is the smallest deployable unit. It contains one or more containers that share storage, network, and settings. Pods are ephemeral - they can be created, destroyed, and replaced as needed.\"},\n",
    "    {\"instruction\": \"What's the difference between Docker and Kubernetes?\", \"input\": \"\", \"output\": \"Docker packages applications into containers. Kubernetes orchestrates containers across clusters. Think of Docker as the container runtime and Kubernetes as the container manager at scale.\"},\n",
    "    {\"instruction\": \"What is infrastructure as code?\", \"input\": \"\", \"output\": \"Infrastructure as Code (IaC) manages infrastructure through configuration files rather than manual processes. Tools like Terraform, Pulumi, and CloudFormation let you version, review, and automate infrastructure changes.\"},\n",
    "    {\"instruction\": \"Explain blue-green deployment.\", \"input\": \"\", \"output\": \"Blue-green deployment maintains two identical environments: blue (current) and green (new). Traffic switches from blue to green after validation. This enables zero-downtime deployments and quick rollbacks.\"},\n",
    "    {\"instruction\": \"What is a load balancer?\", \"input\": \"\", \"output\": \"A load balancer distributes incoming traffic across multiple servers. It improves availability, scalability, and reliability. Types include Layer 4 (transport) and Layer 7 (application) balancers.\"},\n",
    "    {\"instruction\": \"Explain microservices architecture.\", \"input\": \"\", \"output\": \"Microservices break applications into small, independent services. Each service handles a specific business function, communicates via APIs, and can be deployed separately. Benefits include scalability and flexibility; challenges include complexity and distributed system issues.\"},\n",
    "    {\"instruction\": \"What is observability?\", \"input\": \"\", \"output\": \"Observability is the ability to understand a system's internal state from its outputs. The three pillars are: metrics (numbers), logs (events), and traces (request paths). It's essential for debugging distributed systems.\"},\n",
    "]\n",
    "\n",
    "print(f\"DevOps Q&A entries: {len(DEVOPS_QA)}\")\n",
    "\n",
    "# Save as JSONL\n",
    "data_dir = Path(\"../data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(data_dir / \"devops_qa.jsonl\", \"w\") as f:\n",
    "    for entry in DEVOPS_QA:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to {data_dir / 'devops_qa.jsonl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and split\n",
    "with open(data_dir / \"devops_qa.jsonl\") as f:\n",
    "    all_data = [json.loads(line) for line in f]\n",
    "\n",
    "train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save splits\n",
    "with open(data_dir / \"train.jsonl\", \"w\") as f:\n",
    "    for entry in train_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "with open(data_dir / \"val.jsonl\", \"w\") as f:\n",
    "    for entry in val_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Validation: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) adds small trainable matrices instead of modifying all weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                    # Rank of adaptation\n",
    "    lora_alpha=32,          # Scaling factor\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to adapt\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check trainable parameters (without loading full model)\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Print the number of trainable parameters.\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "\n",
    "# This would work with a loaded model:\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Template for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example, tokenizer):\n",
    "    \"\"\"Format example for instruction tuning.\"\"\"\n",
    "    if example[\"input\"]:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "example = DEVOPS_QA[0]\n",
    "print(format_prompt(example, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Your Tasks\n",
    "\n",
    "### Task 1: Expand the Dataset\n",
    "Add at least 20 more DevOps Q&A entries to make a meaningful training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more entries\n",
    "more_entries = [\n",
    "    # {\"instruction\": \"...\", \"input\": \"\", \"output\": \"...\"},\n",
    "]\n",
    "\n",
    "# Combine and save\n",
    "# full_dataset = DEVOPS_QA + more_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Prepare a Different Domain\n",
    "Create a dataset for a domain you're interested in (e.g., customer support, code generation, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your custom dataset\n",
    "CUSTOM_DATASET = [\n",
    "    # Add your entries\n",
    "]\n",
    "\n",
    "# Save it\n",
    "# with open(data_dir / \"custom.jsonl\", \"w\") as f:\n",
    "#     for entry in CUSTOM_DATASET:\n",
    "#         f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Document Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a README for your data\n",
    "data_readme = \"\"\"# Dataset Documentation\n",
    "\n",
    "## devops_qa.jsonl\n",
    "- **Purpose**: DevOps Q&A fine-tuning\n",
    "- **Format**: Instruction-tuning JSONL\n",
    "- **Entries**: {len(DEVOPS_QA)}\n",
    "- **Split**: 80/20 train/val\n",
    "\n",
    "## Schema\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"The task or question\",\n",
    "  \"input\": \"Optional context (can be empty)\",\n",
    "  \"output\": \"The expected response\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Source\n",
    "Curated from DevOps documentation and common interview questions.\n",
    "\"\"\"\n",
    "\n",
    "with open(data_dir / \"README.md\", \"w\") as f:\n",
    "    f.write(data_readme)\n",
    "\n",
    "print(\"Data README created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìù Reflection\n",
    "\n",
    "1. Why use LoRA instead of full fine-tuning?\n",
    "2. What makes a good fine-tuning dataset?\n",
    "3. How would you evaluate a fine-tuned model?\n",
    "4. What are the risks of fine-tuning on small datasets?\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Run `scripts/train_lora.py` to train your model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
