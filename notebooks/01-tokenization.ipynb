{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01 â€¢ Tokenization & Context Windows\n",
    "\n",
    "**Goal:** Understand how text gets turned into tokens, why token counts matter, and explore different tokenizers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you need to install dependencies\n",
    "# !pip install transformers tiktoken sentencepiece\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Token?\n",
    "\n",
    "Tokens are the basic units that LLMs process. They can be:\n",
    "- Whole words\n",
    "- Subwords (parts of words)\n",
    "- Characters\n",
    "- Special tokens (`<|endoftext|>`, `<pad>`, etc.)\n",
    "\n",
    "Different models use different tokenizers. Let's compare a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts to experiment with\n",
    "SAMPLES = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"LLMs are transforming how we interact with computers.\",\n",
    "    \"Tokenization splits text into smaller pieces for processing.\",\n",
    "    \"Specialized terms like RAG, LoRA, and fine-tuning are common in ML.\",\n",
    "    \"Ø°Ù‡ Ø§Ù„Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\",  # Arabic\n",
    "    "\"ã“ã®æ–‡ç« ã¯æ—¥æœ¬èªžã§ã™ã€‚\",  # Japanese\n",
    "    "\"100 + 200 = 300; 3.14159 * 2 = 6.28318\",  # Numbers and math\n",
    "]\n",
    "\n",
    "print(f\"Number of samples: {len(SAMPLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Tokenizers\n",
    "\n",
    "Let's load different tokenizers and see how they handle the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers for different models\n",
    "tokenizers = {\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"GPT-4 (cl100k)\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"Llama-3\": AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True),\n",
    "}\n",
    "\n",
    "print(\"Tokenizers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, tokenizer_name, tokenizer):\n",
    "    \"\"\"Count tokens for a given text and tokenizer.\"\"\"\n",
    "    if tokenizer_name == \"GPT-4 (cl100k)\":\n",
    "        return len(tokenizer.encode(text))\n",
    "    else:\n",
    "        return len(tokenizer.encode(text))\n",
    "\n",
    "# Compare token counts\n",
    "results = []\n",
    "for sample in SAMPLES:\n",
    "    row = {\"Text\": sample[:50] + \"...\" if len(sample) > 50 else sample}\n",
    "    for name, tok in tokenizers.items():\n",
    "        row[f\"{name} tokens\"] = count_tokens(sample, name, tok)\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecting Individual Tokens\n",
    "\n",
    "Let's see exactly how a string gets split into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokens(text, tokenizer_name, tokenizer):\n",
    "    \"\"\"Display how text is tokenized.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Tokenizer: {tokenizer_name}\")\n",
    "    print(f\"Text: {text!r}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if tokenizer_name == \"GPT-4 (cl100k)\":\n",
    "        tokens = tokenizer.encode(text)\n",
    "        token_strs = [tokenizer.decode([t]) for t in tokens]\n",
    "    else:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        token_strs = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    \n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"\\nTokens:\")\n",
    "    for i, (tid, tstr) in enumerate(zip(tokens, token_strs)):\n",
    "        print(f\"  {i:3d} | ID: {tid:6d} | {tstr!r}\")\n",
    "\n",
    "# Try it on a few samples\n",
    "test_text = \"Tokenization is fascinating! Let's explore LLMs.\"\n",
    "for name, tok in tokenizers.items():\n",
    "    show_tokens(test_text, name, tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Window Limits\n",
    "\n",
    "Different models have different context window sizes. Let's explore what happens as text gets longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model context limits\n",
    "CONTEXT_LIMITS = {\n",
    "    \"GPT-3.5-turbo\": 4096,\n",
    "    \"GPT-4-turbo\": 128000,\n",
    "    \"Claude-3-Haiku\": 200000,\n",
    "    \"Llama-3-8B\": 8192,\n",
    "}\n",
    "\n",
    "# Generate progressively longer texts\n",
    "base_sentence = \"The transformer architecture revolutionized NLP. \"\n",
    "\n",
    "print(\"Words vs Tokens (GPT-4 tokenizer):\\n\")\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for multiplier in [1, 10, 50, 100, 500]:\n",
    "    text = base_sentence * multiplier\n",
    "    word_count = len(text.split())\n",
    "    token_count = len(enc.encode(text))\n",
    "    \n",
    "    # Check against limits\n",
    "    fits_in = [model for model, limit in CONTEXT_LIMITS.items() if token_count < limit]\n",
    "    \n",
    "    print(f\"Words: {word_count:6d} | Tokens: {token_count:7d}\")\n",
    "    print(f\"  Fits in: {', '.join(fits_in)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸŽ¯ Your Tasks\n",
    "\n",
    "Complete these exercises to solidify your understanding.\n",
    "\n",
    "### Task 1: Token Efficiency\n",
    "Find which of these formats produces fewer tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "formats = [\n",
    "    '{\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}',\n",
    "    'Name: Alice\\nAge: 30\\nCity: New York',\n",
    "    'Alice is 30 years old and lives in New York.',\n",
    "]\n",
    "\n",
    "# TODO: Count tokens for each format and print the results\n",
    "for fmt in formats:\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Token Frequency Analysis\n",
    "Analyze the most common tokens in a longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paste a longer text (article, documentation, etc.)\n",
    "long_text = \"\"\"\n",
    "PASTE YOUR TEXT HERE\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Tokenize and find the 10 most common tokens\n",
    "# Hint: Use Counter from collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Cost Estimation\n",
    "Calculate the approximate cost for processing text with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing per 1M tokens (as of 2024, check current prices!)\n",
    "PRICING = {\n",
    "    \"GPT-4-turbo\": {\"input\": 10.00, \"output\": 30.00},\n",
    "    \"GPT-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50},\n",
    "    \"Claude-3-Haiku\": {\"input\": 0.25, \"output\": 1.25},\n",
    "}\n",
    "\n",
    "def estimate_cost(text, model, output_tokens_estimate=100):\n",
    "    \"\"\"Estimate cost for processing text with a model.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    input_tokens = len(enc.encode(text))\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * PRICING[model][\"input\"]\n",
    "    output_cost = (output_tokens_estimate / 1_000_000) * PRICING[model][\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens_estimate,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": input_cost + output_cost,\n",
    "    }\n",
    "\n",
    "# TODO: Test with different texts and models\n",
    "test_text = \"Explain quantum computing in simple terms.\"\n",
    "# result = estimate_cost(test_text, \"GPT-4-turbo\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ“ Reflection\n",
    "\n",
    "Answer these questions in your journal or fork of this repo:\n",
    "\n",
    "1. Why do different tokenizers produce different token counts for the same text?\n",
    "2. How does tokenization affect the cost of using LLM APIs?\n",
    "3. What strategies can you use to reduce token usage in prompts?\n",
    "4. Why might code or non-English text require more tokens?\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Move on to `02-prompt-lab.ipynb` to experiment with prompt engineering!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
